---
layout: post
title: "[논문리뷰] NeRF(Representing SCenes as Neural Radiance Field) 리뷰"
date: 2023-08-01
categories: Paper
photos: /assets/post_images/titleImg/iln.png
tags: [NeRF, 논문리뷰, implicitfunction] 
description: " NeRF(Representing SCenes as Neural Radiance Field) 꼼꼼히 리뷰하며 논문 읽는 능력을 길러보자"
published: true
use_math: true
---

<br/>

<br/>



# 논문 리뷰 NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

NeRF는 3d scene reconstruction 분야에 혁신적인 논문이다. 이제는 논문의 제목이었던 NeRF가 아예 하나의 분야가 되었으며, 매년 cvpr , iccv 와 같은 학회에는 NeRF와 관련된 수많은 논문이 쏟아져나온다. 이러한 NeRF가 어떤 모델인지 자세하게 알아 보도록 하자

[implicit function](https://donguk071.github.io/liif/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2023/04/16/implicit/)에 관한 리뷰글을 보고 오는 것을 추천한다. 


## introduction

![Untitled](/assets/post_images/NeRF/Untitled.png)

NeRF는 특정 view에서 본 물체의 이미지들을 이용하여, 그 장면 혹은 공간을 나타내는 implicit function을 정의하고 novel view에서의 volume rendering이 가능하게 해주는 모델이다.

NeRF 는 추가적 특징으로는 일반적인 딥러닝 모델들과 다르게 1scene(case) 1model의 특징을 갖으며 이러한 특징 때문에 overfitting model이라고도 부르기도 한다.

Computer vision 에서 novel view synthesis 와 3d scene reconstruction 이라는 챌린징한 task를 딥러닝으로 학습한 implicit function을 통한 modeling으로 효과적으로 풀어내었다는 의의도 존재한다.

## 2.NeRF overview

NeRF는 기본적으로 volume rendering 을 기반으로 하기 때문에 point cloud로부터 scene을 생성하는 것을 목표로 한다. 다시 말해 NeRF는 궁극적으로 적당한 위치에 배치된 point cloud에  적당한 색상을 더해주는 모델이라고 이해 할 수 있다.

![Untitled](/assets/post_images/NeRF/Untitled%201.png)

NeRF 학습에 전체적인 model flow는 위와 같다. 

1. 한 카메라의 position에서 $(\theta,\phi)$ 로 정의된 각 pixel에 방향으로 ray를 생성 후 ray 위 점을 sampling하여 point $(x,y,z)$를 결정한다. 
2. $(x,y,z,\theta,\phi)$가 MLP를 거쳐 RGB 색상과, 밀도 혹은 투과도 $\sigma$ 값을 예측하도록 한다. 
3. 오류 역전파를 위해서는 다른 view에서의 GT영상과 예측한 point cloud를 통해 rendering된 영상간 MSE를 사용하여 모델이 학습되는 과정을 거친다.

위 과정을 대략 50000 iteration 이상 반복한다면 완벽한 pointcloud가 생성, 최종적으로 다양한 novel view에서의 volumn rendering된 영상을 얻을 수 있다.

이후 내용은 point sampling과 volumn rendering 부분으로 나누어서 설명하겠다. 하지만 두 network는 분리된 작업이 아닌 MLP학습 과정에 동시 업데이트되는 부분이다.

## point sampling

NeRF를 소개하면서 적당한 위치에 point cloud를 생성하고 point에 적절한 색상과 투과도를 정해주는 모델이라고 설명하였다. 해당 관점에서 적당한 위치에 point cloud를 생성하는 방법 먼저 설명을 해보도록 하겠다.

![Untitled](/assets/post_images/NeRF/Untitled%202.png)

Point 배치 즉 ray상의 point를 sampling하기 위해 coarse와 fine network가 존재합니다.

Coarse network : Coarse network는 uniform 하게 ray상에 bin을 나누고 Point 들은 각 bin안에서 random 한 위치에 생선된다.

Fine network :  추출된 point 중 point density(σ) 값이 높은 point를 주변에 추가적인 sampling을 진행한다.

이런식으로 sampling을 설계한 이유는 다음과 같이 생각할 수 있다. 

먼저 물체의 경계가 어디있을지 모르기 때문에 일단 균등하게 점들을 sampling 하고 density 가 급변하는 부분 탐색한다. point들을 각 bin에서 random한 위치에 뽑으면서 overfitting을 방지한다.

σ값이 높다면 주변에 물체의 경계가 있을 가능성이 높기 때문 추가적으로 주변점을 sampling 하는 과정을 거친다.

## Volumn rendering

다음은 sampling된 point에 색상 및 투과도를 학습하고 volumn rendering을 진행하는 수식을 알아보겠다.

![Untitled](/assets/post_images/NeRF/Untitled%203.png)

NeRF가 적당한 RGBσ 값을 구하는 과정은 위와 같다. NeRF는 위와 같은 수식을 통해 RGBσ를 구하고 volumn rendering을 진행한다. 먼저 t는 각각의 ray에서 sampling된 point를 의미하며, 각 point는 앞서 설명한것과 같이 coarse와 fine network를 거쳐 추출된다. 

![Untitled](/assets/post_images/NeRF/Untitled%204.png)

다음으로 Ray를 정의하는 함수를 보자 

Orientation 과 direction은 카메라 별로 공유된다. O는 카메라 translation 정보 d는 rotation과 방향가중치 t을 곱한 값으로 정의된다. 

* t는 정확히는 rendering 거리 혹은 point를 생성할 거리정보를 의미 결국 point의 거리정보

![Untitled](/assets/post_images/NeRF/Untitled%205.png)

T(transmittance) 즉 point 까지의 누적 투과도를 의미한다. ray가 현재 물체가 있는 지점까지 도달할 확률 혹은 빈공간일 확률로 생각할 수도 있다.

ray가 되면서 앞부분 밀도혹은 누적투과도함수 T 의 결과가 크면 이후 포인트 학습에 큰 영향을 주지 않도록 설계되어있다.

![Untitled](/assets/post_images/NeRF/Untitled%206.png)

quary point (t)까지의 누적 투과도로 해당 색상을 얼마나 사용할지 계산한다.
최종 rendering되는 색상은 (누적투과도 x 현지점에투과도 x 현지점color값)

## Additional skills : discretize

![Untitled](/assets/post_images/NeRF/Untitled%207.png)

마지막으로 전체적인 흐름을 파악하자면 volume rendering시 한 픽셀에 대하여  quary pixel의 색상을 정하기 위해 ray상 모든 point의 누적 투과도를 이용하여 point별 색상을 얼마나 사용할지 계산하는 과정을 거쳐 최종적인 pixel의 색상을 결정한다. 즉, 누적투과도 x 현지점에투과도 x 현지점color값으로 이루어지고 이 계산식으로 총 transmittance, density($\sigma$ ), color($c$) 총 3개의 함수로 분리되어 하나의 implicit function을 이룬다고 볼수있다.

하지만 $C(r)$ 의 적분식은 continuous space에서의 수식이다. 이를 컴퓨터로 연산하기 위해 continuous space에서의 수식을 discrete space에서의 연산으로 변환이 필요하다.

해당 방법을 위해 Optical Models for Direct Volume Rendering(1995) 논문을 인용하여 아래와 같은 수식으로 변환한다.

## Additional skills : poistional encoding

![Untitled](/assets/post_images/NeRF/Untitled%208.png)

MLP model(Implicit function)이 어케 학습되는지 보자면, 10개의 sin,cos 주파수를 이용하여 input x,y,z 정보를 차원 팽창한다.(푸리에변환) 결과 60차원으로 팽창하였다. $(\theta,\phi)$ 또한 마찬가지로 이러한 과정을 거쳐 MLP의 input으로 사용된다. 이 기법은 transformer에 postional encoding과 같기 때문에 해당 네이밍을 가져다 사용한것 같다. 하지만 그 역할은 transformer 에 그것과 다르다.

그 차이는 Fourier features let networks learn high frequency functions in low dismenstional domains 논문을 통해 알 수있는데,  해당 논문을 보면 Fourier features을 이용한 데이터 고차원 투영이 high frequence를 잘찾아내는 것을 확인할 수 있다.




## Conclusion
<br/>

위 내용이 NeRF 에 전부이다. 시작할때도 말했지만 MLP를 이용하여 색과 밀도(투과도)를 찾아주는 함수를 만들고 해당 함수들을 이용하여 기존 volumn rendering 수식에 적용 implicit한 3d scene reconstuction을 하는것이다. (말이 복잡한가…)


타 블로그와 차별성있게 최대한 자세히 설명해 보려했지만 역시 이해하려면 정말 많은 배경지식이 필요한 논문이다. 특히 마지막 postional encoding 관련하여서도 논문이 존재하는데 내용이 무척 어렵다. 해당 논문도 조만간 리뷰하도록 하겠다.

타 블로그와 차별성있게 최대한 자세히 설명해 보려했지만 역시 이해하려면 정말 많은 배경지식이 필요한 논문이다. 특히 마지막 postional encoding 관련하여서도 논문이 존재하는데 내용이 무척 어렵다. 해당 논문도 조만간 리뷰하도록 하겠다.

<br/>

---

Reference

1) [NeRF paper and page](https://www.matthewtancik.com/nerf)


<br/>


궁금한 점이 있다면 남겨주세요! 함께 고민해 보겠습니다.

------------------------