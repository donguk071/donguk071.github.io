---
layout: post
title: "[프로젝트] Greedot"
date: 2024-01-22
categories: Project
photos: 
tags: [프로젝트] 
description: "Greedot 프로젝트의 진행 일지입니다"
published: true
use_math: true
---

<br/>
<br/>


# Greedot


<br/>

Greedot은 아이들이 그린 그림에 대한 메타휴먼을 만들어 교육, 치료등 다양한 분야로의 확장성을 갖는 프로젝트이다. 더욱 자세한 소개는 [Greedot 발표자료]()를 통해 확인 가능하다.

<br/>

## 1주차(24.01.22)

<br/>

**금주 진행 사항**

2D 그림을 리깅하기 위해 [Animate drawings]()를 참고했다.
논문에서 설명하는 방법을 통하여 segmentation 을 구현하였으며 해당 방법은 NON-DL approach 이다.

**차주 진행 예정**

팀내 따로 front 담당이 없기에 
당분간 fluuter를 통한 front 개발을 진행할 것으로 생각된다.

## 2주차(24.01.29)

<br/>


<br/>

여기까지 정리하여 보면 투과도 즉 opacity값은 0\~1 사이의 값을 갖기 때문에 확률값이라 볼 수 있다. 
해당 지점에 투명도가 낮을 수록 정보량을 클것이고, 불투명한 부분일수록 정보량은 적을 것이다. 
따라서 해당 값이 작아 지도록 학습하면 적합한 point의 위치를 찾는데 도움을 줄 수 있다.


다만 ray 가 아무 물체도 hit 하지않는다면 p식의 분모가 작아지기 때문에 낮은 정보량를 갖는다. 
따라서 아래와 같은 별도 과정을 추가해주어 물체를 지나는지 확인한다.
<br/>

<center>
$ 
    Q(\mathrm{r})=\sum_{i=1}^N 1-\exp \left(-\sigma_i \delta_i\right)
$
</center>

<br/>

ray 상에 물체가 존재하는지 판단하기 위해 투과도에 누적합을 사용하며 이를 $Q(\mathrm{r})$로 정의한다.


$Q(\mathrm{r})$을 통해 얻은 값이 $\epsilon$이하의 entropy의 누적합을 갖는다면 
물체를 지나는 ray가 아니라고 판단된다면 학습을 진행하지 않는다.
<br/>

<center>
$ 
    \mathcal{L}_{\text {entropy }}=\frac{1}{\left|\mathcal{R}_s\right|+\left|\mathcal{R}_u\right|} \sum_{\mathbf{r} \in \mathcal{R}_s \cup \mathcal{R}_u} M(\mathrm{r}) \odot H(\mathrm{r})
$
</center>

<br/>

앞서 구한 함수들을 이용하여 위와 같은 loss function을 정의 한다


## KL divergence

<br/>

마지막으로 ray entropy를 사용할 경우 few shot image에 overfitting되는 이슈가 있다고 한다.
따라서 약간의 불확실성을 주기 위해 KL divergence의 개념을 추가한다.
-5\~5 도 정도 벗어난 ray와 원본 ray를 이용하여 둘 사이 KL divergence를 최소화하는 loss term을 더해준다.
(두 분포 사이를 계산하는 수치)
즉, 기존 NeRF 에 input으로 들어가던 $(\theta,\phi)$ 값을 -5~+5도 정도 조절하여 넣어준 것이다.



<br/>

개인적인 해석을 한번 해보자면 약간의 데이터 증강 강제적으로 오차를 주는 과정 진행하여
overfitting을 해결한것 같다. 
비슷한 시도를 머신러닝 알고리즘에서 본것이 있던것 같다. 
해당 방법이 좋은 방법이라고 생각되지는 않지만,
NeRF에 경우 positional encoding을 거치기 때문에 저러한 variation 주어도 학습결과를
망치지 않고 overfitting을 방지하는게 아닐까 생각한다.

<br/>

<center>
$ 
 \mathcal{L}_{\text {total }}=\mathcal{L}_{\text {RGB }}+\lambda_1 \mathcal{L}_{\text {entropy }}+\lambda_2 \mathcal{L}_{\mathrm{KL}}
$</center>

<br/>

앞서 설명한 각정 loss function을 이용하여 최종적인 loss function을 정의한다.
각 loss 대한 weighted sum 을 진행한다. 


정말 간단하지만 위 내용이 InfoNeRF에 전부이다. 아이디어는 간단하지만 내용은 수학적인 내용이 많아 쉽지만은 않다.

loss function을 잘 설계한 것 많으로 이전 모델을 상당히 개선한 모습이 인상적이다. 
아직 공부가 부족해 InfoNeRF의 loss term이 이후 NeRF 모델에 basic하게 적용되는지는 모르겠지만, 실제 코드를 돌려본 결과 custom dataset에 대해 각종 파라미터에 매우 민감한 것 같다.


---

Reference

1) [infoNeRF paper and page](https://cv.snu.ac.kr/research/InfoNeRF/)


<br/>


궁금한 점이 있다면 남겨주세요! 함께 고민해 보겠습니다.

------------------------